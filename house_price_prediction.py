# -*- coding: utf-8 -*-
"""House_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WnRTDlHdI2lzIJCfH9nvSm0PLp9lrWEW
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

data =pd.read_csv('/content/melb_data.csv')

data.shape

data.head()

data.info()

data.isna().sum()

data.describe()

data['Suburb'].nunique()

data['Regionname'].unique()

data['Method'].unique()

data['CouncilArea'].nunique()

numeric_cols =data.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols =data.select_dtypes(include=['object']).columns.tolist()


print("Numeric Columns:", numeric_cols)
print("Categorical Columns:", categorical_cols)

missing_pct = (data.isnull().mean() * 100).sort_values(ascending=False)
print("\nMissing Values %:\n", missing_pct)

correlation_with_price = data[numeric_cols].corr()['Price'].abs().sort_values(ascending=False)
print("\nTop Correlations with Price:\n", correlation_with_price)

data['BuildingArea'].skew()

from sklearn.impute import SimpleImputer
B_imp =SimpleImputer(strategy='median')


data['BuildingArea'] =B_imp.fit_transform(data[['BuildingArea']])

print(data['BuildingArea'].isnull().sum())

data['BuildingArea'].skew()

data['YearBuilt'].skew()

data['YearBuilt'] =B_imp.fit_transform(data[['YearBuilt']])

print(data['YearBuilt'].isnull().sum())

data.isna().sum()

data['Car'].skew()

# use median imputation in car column
data['Car'] =B_imp.fit_transform(data[['Car']])

data['Car'].skew()

data['Car'].isna().sum()

data['CouncilArea'].mode()

#use mode for council area
Mode_imp =SimpleImputer(strategy='most_frequent')

data['CouncilArea'] =Mode_imp.fit_transform(data[['CouncilArea']]).ravel()

data.isna().sum()

data.nunique().sort_values(ascending=False)

high_cardinality_cols = [col for col in data.columns if data[col].nunique() > 50 and data[col].dtype == 'object']
high_cardinality_cols

low_cardinality_cols = [col for col in data.columns if data[col].nunique() < 50 and data[col].dtype == 'object']
low_cardinality_cols

data['Type'].unique()

data =pd.get_dummies(data ,columns=['Type'] ,drop_first=True)

data =pd.get_dummies(data ,columns=['Method'] ,drop_first=True)

data['CouncilArea'].nunique()

# freq encoding for Suburb
suburb_freq = data['Suburb'].value_counts()
data['Suburb'] = data['Suburb'].map(suburb_freq)

suburb_freq = data['CouncilArea'].value_counts()
data['CouncilArea'] = data['CouncilArea'].map(suburb_freq)

data.drop('Address', axis=1, inplace=True)

data.drop('SellerG', axis=1, inplace=True)

data.drop('Regionname', axis=1, inplace=True)

data['Year'] = pd.to_datetime(data['Date'], dayfirst=True).dt.year
data['Month'] = pd.to_datetime(data['Date'], dayfirst=True).dt.month
data.drop('Date', axis=1, inplace=True)

data.shape

data.head()

# train test split

x =data.drop('Price', axis =1)
y =data['Price']

x_train, x_test ,y_train ,y_test =train_test_split(x , y ,test_size=0.2 ,random_state=42)

x_train.shape ,x_test.shape ,y_train.shape ,y_test.shape

dt_reg =DecisionTreeRegressor(random_state=42)

dt_reg.fit(x_train, y_train)

y_pred = dt_reg.predict(x_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("MAE:", mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R²:", r2)

from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': [None, 'sqrt', 'log2']
}

grid_search = GridSearchCV(estimator=dt_reg, param_grid=param_grid,
                           cv=5, scoring='r2', n_jobs=-1)

grid_search.fit(x_train, y_train)

print("Best Parameters:", grid_search.best_params_)

y_pred = grid_search.predict(x_test)

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("R² Score:", r2)
print("MAE:", mae)
print("RMSE:", rmse)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor

rf_reg = RandomForestRegressor(random_state=42)


param_dist = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    rf_reg,
    param_distributions=param_dist,
    n_iter=30,
    cv=3,
    n_jobs=-1,
    random_state=42,
    verbose=2
)


random_search.fit(x_train, y_train)


print("Best Parameters:", random_search.best_params_)

best_rf = random_search.best_estimator_
y_pred = best_rf.predict(x_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"MAE: {mae}")
print(f"RMSE: {rmse}")
print(f"R² Score: {r2}")

"""Feature Importance"""

importances = best_rf.feature_importances_
features = x_train.columns

feature_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df["Feature"][:15], feature_importance_df["Importance"][:15])
plt.gca().invert_yaxis()
plt.title("Top 15 Important Features - Random Forest")
plt.xlabel("Importance")
plt.show()

print(feature_importance_df.head(15))